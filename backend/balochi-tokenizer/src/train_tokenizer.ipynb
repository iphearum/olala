{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets tokenizers\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"balochiml/balochi-language-data\", data_dir=\"data\", cache_dir=\"../data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the processed data without English characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset\n",
    "!sh download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_txt_file_paths(directory):\n",
    "    txt_file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                txt_file_paths.append(file_path)\n",
    "    return txt_file_paths\n",
    "\n",
    "\n",
    "# Replace \"directory_path\" with the actual path of the directory you want to search\n",
    "directory_path = \"../data/raw_text\"\n",
    "txt_paths = get_txt_file_paths(directory_path)\n",
    "\n",
    "len(txt_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(file_path):\n",
    "    # Open the file and read it into memory\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Remove English-language characters and numbers\n",
    "    text = re.sub(r\"[a-zA-Z0-9]\", \"\", text)\n",
    "\n",
    "    # Remove any excess whitespace\n",
    "    text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in txt_paths:\n",
    "    cleaned_text = clean_text(path)\n",
    "\n",
    "    # write the cleaned text to a new file with an incremented filename\n",
    "    # write the files all into the '../data/processed_text' directory\n",
    "    with open(\n",
    "        f'../data/processed_text/{path.split(\"/\")[-1]}', \"w\", encoding=\"utf-8\"\n",
    "    ) as file:\n",
    "        file.write(cleaned_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer using ğŸ¤— Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    min_frequency=2,\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the txt files in\n",
    "# '/Users/strickvl/balochi/balochi-tokenizer/data/processed_text'\n",
    "\n",
    "processed_files = get_txt_file_paths(\"../data/processed_text\")\n",
    "assert len(processed_files) == len(txt_paths)\n",
    "len(processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(processed_files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"../models/30k-balochi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"../models/30k-balochi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Â  Â  Â  Ø¢ÛŒÚ©Â  Ø¬Ù†Ø§ÙˆØ±Û’ Ø§ÙØªÛ”Â  Ù„Ú¾ØªÛ’ Ú¯Ø´ÛŒØª Ø¢ Ø³Ú©ÛŒÚº Ú©Ø§Ø±Ø²ÙˆØ§Ù„Û’ Ø§Øª Ú©Û Ø§Ú¯Ø§Úº Ø¢Ø²Ø§ØªÛŒ Ø¯ÛŒÚ¯ Ø¨Û Ø¨ÛŒØªØŒ Ø¨Ø§Ø²Ø§Ø±Ø¡ÙØŒ Ù„ÙˆÚ¯Û’ Ø¡ÙØŒ Ø¬Ø§Ú¯Ø§Û ÛŒÛ’Â  Ø¡ÙØŒØ¯Ù¾ØªØ± Ø¡ Ù Ú©Ø§Ø±Ú¯Ø³ ÛŒÛ’Â  Ø¡Ù ÛŒØ§ Ú¾Ø± Ú¾Ù…Ø§ Ø¬Ø§Ú¯Ø§Û Ø¡Ù Ú©Û Ø´ÙØª Ú©Ù†Øª Ù…Ø²Ù†ÛŒÚº Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒ Ú©Ù†ØªÛ”Ú¯ÙˆÚº Ú¾Ø± Ú©Ø³ Ø¡Ù Ø¬Ù†Ú¯ Ø¡ Ù Ù…Ú‘ Ø¨ÛŒØªÛ”Ú¯Ø¯Ø¡ Ù Ù¾Ú†Ø§ÚºÂ  Ú†Ù†Úˆ Ú†Ù†Úˆ Ø¡ Ù Ø±Ø§Ú‘ Ø±Ø§Ú‘ Ú©Ù†ØªØŒÚ©Ø§Ú¯Ø¯ Ø¡ Ù ÙˆØ§Ù†Ú¯ÛŒØ§Úº ÙˆØ§Ø±Øª Ø¡ Ù Ø¢Ø¯Ø±Ø§Û Ú©Ù†ØªÛ”ÙˆØ±Ú¯ÛŒ Ú†ÛŒØ²Ø§Úº Ø§Ú¯Ø§Úº ÙˆØ§Ø±Øª Ù†Ú©Ù†Øª Ø¢Ú¾Ø§Úº Ú¯Ù¹ Ù¾Ø§Ú†ÛŒØª Ú¾Ø±Ø§Ø¨ Ú©Ù†ØªÛ”Ø§ÛŒÙ†Ø¯Ú¯Û Ø¬Ù†Ø§ÙˆØ± Ú†Û Ø¨Ù†Ø¯Ø§Øª Ø¡ Ù Ø§ÛŒØ´ÛŒ Ø¡Ù Ú©Ø§Ø²ÙˆØ§Ù„ÛŒØ§Úº Ú†Û ÙˆØªØ§ Ø¯ÛŒØ± Ø¯Ø§Ø±Ú¯ Ø¡Ù Ú©ÙˆØ´Ø³Øª Ú©Ù† Ø§ÙÙ†ØªÛ” Ú†ÛŒØ§ Ú©Û Ø¢ Ø¨Ø§Ø²ÛŒÚº Ø¯Ú¯Û Ú¾Ø±Ø§Ø¨ÛŒ Ø¡ Ù Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒ Ú¾Ù… Ú©Ù†ØªØŒÙ¾Ù…ÛŒØ´Ú©Ø§ Ú©Ø³Ø§Ù†ÛŒÚº Ø¬Ù†Ø§ÙˆØ±Â  Ø¨Ø§Ù„ÛŒ Ù…ÙØ±Ú¯ØŒÚ©ÙˆÛ Ù¾Ø§Ú†Ù†ØŒØ¢Ø³Ú© Ø¡ Ù Ø§ÛŒÙ†Ø¯Ú¯Û Ú©Ø³Ø§Ù† Ú©Ø³Ø§Ù†ÛŒÚº Ø¬Ù†Ø§ÙˆØ±Ú†Ø± Ø¢Ø¦ÛŒ Ø¡Ù Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒØ§Ù†ÛŒ Ø³ÙˆØ¨ Ø¡Ù Ø¢Ø¦ÛŒ Ø¡Ù Ú†Û Ø³Ú© Ø¨Ø§Ø² Ø´Ø²Ø§Ø± Ø§ÙÙ†Øª Û”\".replace(\n",
    "    \"\\xa0\", \"\"\n",
    ")\n",
    "sample_sentence = sample_text.split(\"Û”\")[2]\n",
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(sample_sentence).tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a custom tokenizer using Spacy and FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "files = get_text_files(\"../data/processed_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open().read() for o in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(size: int):\n",
    "    sp = SubwordTokenizer(vocab_sz=size)\n",
    "    sp.setup(txts)\n",
    "    return \" \".join(first(sp([txt]))[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "balochi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
