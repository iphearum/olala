{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llm-axe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_axe.agents import FunctionCaller, OnlineAgent\n",
    "from llm_axe.models import OllamaChat\n",
    "import time, json, ollama\n",
    "\n",
    "# We define the functions that we want the llm to be able to call. \n",
    "# Note that documentation is not required, but should be used \n",
    "#   to help the llm for understanding what each function does.\n",
    "# Specifying parameter types is optional but highly recommended\n",
    "def get_time():\n",
    "    return time.strftime(\"%I:%M %p\")\n",
    "\n",
    "def get_date():\n",
    "    return time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def get_location():\n",
    "    return \"USA\"\n",
    "\n",
    "def add(num1:int, num2:int):\n",
    "    return num1 + num2\n",
    "\n",
    "def multiply(num1:int, num2:int):\n",
    "    return num1 * num2\n",
    "\n",
    "def get_distance(lat1:int, lon1:int, lat2:int, lon2:int):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two points on the Earth's surface using the Haversine formula.\n",
    "    :param lat1: latitude of point 1\n",
    "    :param lon1: longitude of point 1\n",
    "    :param lat2: latitude of point 2\n",
    "    :param lon2: longitude of point 2\n",
    "    \"\"\"\n",
    "    return(lat1, lon1, lat2, lon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaChat(model=\"qwen2\")\n",
    "fc = FunctionCaller(llm, [get_time, get_date, get_location, add, multiply, get_distance], temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama rm funcall\n",
    "!ollama create funcall -f Makefuncall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I have 500 coins, I just got 200 more. How many do I have?\"\n",
    "new_prompt = fc.get_prompt(question=prompt)\n",
    "new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollm = ollama.chat(model=\"funcall\", messages=new_prompt, stream=True, options={\n",
    "    \"temperature\": 0.1\n",
    "})\n",
    "\n",
    "for tex in ollm:\n",
    "    print(tex['message']['content'], sep=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# res = ollama.generate(model='openkh', prompt=prompt, stream=True, template=0)\n",
    "# for text in res:\n",
    "#     print(text)\n",
    "result = fc.get_function(prompt)\n",
    "# If no function was found, exit\n",
    "if(result is not None):\n",
    "    print(\"function found\")\n",
    "    func = result['function']\n",
    "    params = result['parameters']\n",
    "    print(func(**params))\n",
    "    print(result['raw_response'])\n",
    "else:\n",
    "    res = ollama.generate(model='qwen2', prompt=prompt, stream=True)\n",
    "    for text in res:\n",
    "        print(tex['message']['content'], sep=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollm = ollama.chat(model=\"qwen2\", messages=new_prompt, stream=True, options={\n",
    "    \"temperature\": 0.1\n",
    "})\n",
    "\n",
    "for tex in ollm:\n",
    "    print(tex['message']['content'], sep=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(func(**params))\n",
    "print(result['parameters'])\n",
    "print(result['prompts'])\n",
    "print(result['raw_response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a bit about this website:  https://toscrape.com/?\"\n",
    "searcher = OnlineAgent(llm)\n",
    "resp = searcher.search(prompt)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    q = ollama.generate(model=\"phi3\", prompt=text, stream=True, keep_alive=-1, options={\n",
    "        \"temperature\": 0.8\n",
    "    })\n",
    "    for tex in q:\n",
    "        print(tex['response'], sep=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = [\n",
    "    \"Hello, Who are you?\",\n",
    "    \"How are you today?\"\n",
    "]\n",
    "\n",
    "for i in chats:\n",
    "    chat(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
