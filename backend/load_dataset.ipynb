{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html?ref=dailydev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets pandas rich pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/balochiml/balochi-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "test_dataset = load_dataset(\"strickvl/isafpressreleases\", split=\"test\")\n",
    "test_df = pd.DataFrame(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Dict, Set, Annotated, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "class EventType(str, Enum):\n",
    "    airstrike = \"airstrike\"\n",
    "    detention = \"detention\"\n",
    "    captureandkill = \"captureandkill\"\n",
    "    insurgentskilled = \"insurgentskilled\"\n",
    "    exchangeoffire = \"exchangeoffire\"\n",
    "    civiliancasualty = \"civiliancasualty\"\n",
    "\n",
    "\n",
    "class Province(str, Enum):\n",
    "    badakhshan = \"badakhshan\"\n",
    "    badghis = \"badghis\"\n",
    "    baghlan = \"baghlan\"\n",
    "    balkh = \"balkh\"\n",
    "    bamyan = \"bamyan\"\n",
    "    day_kundi = \"day_kundi\"\n",
    "    farah = \"farah\"\n",
    "    faryab = \"faryab\"\n",
    "    ghazni = \"ghazni\"\n",
    "    ghor = \"ghor\"\n",
    "    helmand = \"helmand\"\n",
    "    herat = \"herat\"\n",
    "    jowzjan = \"jowzjan\"\n",
    "    kabul = \"kabul\"\n",
    "    kandahar = \"kandahar\"\n",
    "    kapisa = \"kapisa\"\n",
    "    khost = \"khost\"\n",
    "    kunar = \"kunar\"\n",
    "    kunduz = \"kunduz\"\n",
    "    laghman = \"laghman\"\n",
    "    logar = \"logar\"\n",
    "    nangarhar = \"nangarhar\"\n",
    "    nimroz = \"nimroz\"\n",
    "    nuristan = \"nuristan\"\n",
    "    paktya = \"paktya\"\n",
    "    paktika = \"paktika\"\n",
    "    panjshir = \"panjshir\"\n",
    "    parwan = \"parwan\"\n",
    "    samangan = \"samangan\"\n",
    "    sar_e_pul = \"sar_e_pul\"\n",
    "    takhar = \"takhar\"\n",
    "    uruzgan = \"uruzgan\"\n",
    "    wardak = \"wardak\"\n",
    "    zabul = \"zabul\"\n",
    "\n",
    "\n",
    "class TargetGroup(str, Enum):\n",
    "    taliban = \"taliban\"\n",
    "    haqqani = \"haqqani\"\n",
    "    criminals = \"criminals\"\n",
    "    aq = \"aq\"\n",
    "    hig = \"hig\"\n",
    "    let = \"let\"\n",
    "    imu = \"imu\"\n",
    "    judq = \"judq\"\n",
    "    iju = \"iju\"\n",
    "    hik = \"hik\"\n",
    "    ttp = \"ttp\"\n",
    "    other = \"other\"\n",
    "\n",
    "\n",
    "def validate_event_type(value: str):\n",
    "    valid_values = [\n",
    "        \"airstrike\",\n",
    "        \"detention\",\n",
    "        \"captureandkill\",\n",
    "        \"insurgentskilled\",\n",
    "        \"exchangeoffire\",\n",
    "        \"civiliancasualty\",\n",
    "    ]\n",
    "    if value.lower() not in valid_values:\n",
    "        return \"other\"\n",
    "    return value.lower()\n",
    "\n",
    "\n",
    "def validate_province(value: str):\n",
    "    valid_values = [\n",
    "        \"badakhshan\",\n",
    "        \"badghis\",\n",
    "        \"baghlan\",\n",
    "        \"balkh\",\n",
    "        \"bamyan\",\n",
    "        \"day_kundi\",\n",
    "        \"farah\",\n",
    "        \"faryab\",\n",
    "        \"ghazni\",\n",
    "        \"ghor\",\n",
    "        \"helmand\",\n",
    "        \"herat\",\n",
    "        \"jowzjan\",\n",
    "        \"kabul\",\n",
    "        \"kandahar\",\n",
    "        \"kapisa\",\n",
    "        \"khost\",\n",
    "        \"kunar\",\n",
    "        \"kunduz\",\n",
    "        \"laghman\",\n",
    "        \"logar\",\n",
    "        \"nangarhar\",\n",
    "        \"nimroz\",\n",
    "        \"nuristan\",\n",
    "        \"paktya\",\n",
    "        \"paktika\",\n",
    "        \"panjshir\",\n",
    "        \"parwan\",\n",
    "        \"samangan\",\n",
    "        \"sar_e_pul\",\n",
    "        \"takhar\",\n",
    "        \"uruzgan\",\n",
    "        \"wardak\",\n",
    "        \"zabul\",\n",
    "    ]\n",
    "    if value.lower() not in valid_values:\n",
    "        return \"other\"\n",
    "    return value.lower()\n",
    "\n",
    "\n",
    "def validate_target_group(value: str):\n",
    "    valid_values = [\n",
    "        \"taliban\",\n",
    "        \"haqqani\",\n",
    "        \"criminals\",\n",
    "        \"aq\",\n",
    "        \"hig\",\n",
    "        \"let\",\n",
    "        \"imu\",\n",
    "        \"judq\",\n",
    "        \"iju\",\n",
    "        \"hik\",\n",
    "        \"ttp\",\n",
    "        \"other\",\n",
    "    ]\n",
    "    if value.lower() not in valid_values:\n",
    "        return \"other\"\n",
    "    return value.lower()\n",
    "\n",
    "\n",
    "class IsafEvent(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"A title or name for the event which summarises the event as a headline\"\n",
    "    )\n",
    "    text: Optional[str] = Field(description=\"The full text of the press release\")\n",
    "    start_date: date = Field(\n",
    "        description=\"The start date of the event in YYYY-MM-DD format\"\n",
    "    )\n",
    "    event_type: Set[Annotated[str, Field(validator=validate_event_type)]] = Field(\n",
    "        description=\"The event type. Can be multiple types.\"\n",
    "    )\n",
    "    province: Set[Annotated[str, Field(validator=validate_province)]] = Field(\n",
    "        description=\"The province in which the event occurred. Can be multiple provinces.\"\n",
    "    )\n",
    "    target_group: Set[Annotated[str, Field(validator=validate_target_group)]] = Field(\n",
    "        description=\"The group that was targetted during the event. Can be multiple groups.\"\n",
    "    )\n",
    "    min_killed: int = Field(\n",
    "        description=\"The minimum number of people killed during the event\"\n",
    "    )\n",
    "    min_captured: int = Field(\n",
    "        description=\"The minimum number of people captured during the event\"\n",
    "    )\n",
    "    killq: bool = Field(\n",
    "        description=\"Whether someone was killed or not during the event\"\n",
    "    )\n",
    "    captureq: bool = Field(\n",
    "        description=\"Whether someone was captured or not during the event\"\n",
    "    )\n",
    "    killcaptureraid: bool = Field(\n",
    "        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n",
    "    )\n",
    "    airstrike: bool = Field(\n",
    "        description=\"Whether an airstrike was used during the event\"\n",
    "    )\n",
    "    noshotsfired: bool = Field(\n",
    "        description=\"Whether no shots were fired during the event\"\n",
    "    )\n",
    "    min_leaders_killed: int = Field(\n",
    "        description=\"The minimum number of leaders killed during the event\"\n",
    "    )\n",
    "    min_leaders_captured: int = Field(\n",
    "        description=\"The minimum number of leaders captured during the event\"\n",
    "    )\n",
    "    predictions: Dict[str, str] = Field(\n",
    "        default={},\n",
    "        description=\"The predictions from the model. Keys are the model name and the value is the prediction\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "events: List[IsafEvent] = []\n",
    "\n",
    "for i, row in list(test_df.iterrows()):\n",
    "    event_types = set(\n",
    "        eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n",
    "    )\n",
    "    provinces = set(province.strip().lower() for province in row[\"province\"].split(\",\"))\n",
    "    target_groups = set(\n",
    "        target_group.strip().lower() for target_group in row[\"targetgroup\"].split(\",\")\n",
    "    )\n",
    "\n",
    "    events.append(\n",
    "        IsafEvent(\n",
    "            name=row[\"name\"],\n",
    "            text=row[\"text\"],\n",
    "            start_date=row[\"StartDate\"].to_pydatetime().date(),\n",
    "            event_type=event_types,\n",
    "            province=provinces,\n",
    "            target_group=target_groups,\n",
    "            min_killed=int(row[\"minkilled\"]),\n",
    "            min_captured=int(row[\"mincaptured\"]),\n",
    "            killq=row[\"killq\"] == \"true\",\n",
    "            captureq=row[\"captureq\"] == \"true\",\n",
    "            killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n",
    "            airstrike=row[\"airstrike\"] == \"true\",\n",
    "            noshotsfired=row[\"noshotsfired\"] == \"true\",\n",
    "            min_leaders_killed=int(row[\"minleaderskilled\"]),\n",
    "            min_leaders_captured=int(row[\"minleaderscaptured\"]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(events[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = events[0].model_dump_json(exclude={\"text\", \"predictions\"})\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from rich import print\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def query_openai(article_text: str, model: str) -> str:\n",
    "    query = (\n",
    "        f\"The following is a press release issued by ISAF (formerly operating in Afghanistan):\\n{article_text}\\n\\n\"\n",
    "        \"## Extraction request\\n\"\n",
    "        \"Please extract the following information from the press release:\\n\"\n",
    "        \"- The name of the event (summarising the event / text as a headline)\\n\"\n",
    "        \"- The start date of the event\\n\"\n",
    "        \"- The event type(s)\\n\"\n",
    "        \"- The province(s) in which the event occurred\\n\"\n",
    "        \"- The target group(s) of the event\\n\"\n",
    "        \"- The minimum number of people killed during the event\\n\"\n",
    "        \"- The minimum number of people captured during the event\\n\"\n",
    "        \"- Whether someone was killed or not during the event\\n\"\n",
    "        \"- Whether someone was captured or not during the event\\n\"\n",
    "        \"- Whether the event was a so-called 'kill-capture raid'\\n\"\n",
    "        \"- Whether an airstrike was used during the event\\n\"\n",
    "        \"- Whether no shots were fired during the event\\n\"\n",
    "        \"- The minimum number of leaders killed during the event\\n\"\n",
    "        \"- The minimum number of leaders captured during the event\\n\\n\"\n",
    "        \"## Annotation notes:\\n\"\n",
    "        \"- A 'faciliator' is not a leader.\\n\"\n",
    "        \"- If a press release states that 'insurgents' were detained without further \"\n",
    "        \"details, assign a minimum number of two detained. Interpret 'a couple' as \"\n",
    "        \"two. Interpret 'several' as at least three, even though it may sometimes \"\n",
    "        \"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a \"\n",
    "        \"small group', and 'multiple' as denoting at least three, even if they \"\n",
    "        \"sometimes refer to larger numbers. Choose the smaller number if no other \"\n",
    "        \"information is available in the press release to come up with a minimally \"\n",
    "        \"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, \"\n",
    "        \"and 'a large number' as at least five.\\n\\n\"\n",
    "        \"## Example:\\n\"\n",
    "        \"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \"\n",
    "        \"Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \"\n",
    "        \"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \"\n",
    "        \"their position talking on radios today. After gaining positive identification of the insurgent positions, the \"\n",
    "        \"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \"\n",
    "        \"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent \"\n",
    "        \"positions, resulting in several more insurgents being killed.'\\n\\n\"\n",
    "        'Output: `{\"name\":\"Several insurgents killed in '\n",
    "        'Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi'\n",
    "        'n_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"'\n",
    "        ':false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}`'\n",
    "    )\n",
    "\n",
    "    # set up the prediction harness\n",
    "    client = OpenAI(\n",
    "        base_url = 'http://localhost:11434/v1',\n",
    "        api_key='ollama', # required, but unused\n",
    "    )\n",
    "    # client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert at identifying events in a press release. You are precise \"\n",
    "                \"and always make sure you are correct, drawing inference from the text of the \"\n",
    "                \"press release.\\n\\n You always return a JSON string with the following schema: \"\n",
    "                \"## JSON Schema details\\n\"\n",
    "                \"Here is some of the schema for the JSON output string you \"\n",
    "                \"should make use of: event_types = ['airstrike', 'detention', \"\n",
    "                \"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], \"\n",
    "                \"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', \"\n",
    "                \"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', \"\n",
    "                \"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', \"\n",
    "                \"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', \"\n",
    "                \"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', \"\n",
    "                \"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', \"\n",
    "                \"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=1,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = query_openai(events[0].text, \"phi3\")\n",
    "print(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make async work within a notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "async def async_query_openai(\n",
    "    session,\n",
    "    article_text: str,\n",
    "    model: str,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: float = 1.0,\n",
    ") -> str:\n",
    "    query = (\n",
    "        f\"The following is a press release issued by ISAF (formerly operating in Afghanistan):\\n{article_text}\\n\\n\"\n",
    "        \"## Extraction request\\n\"\n",
    "        \"Please extract the following information from the press release:\\n\"\n",
    "        \"- The name of the event (summarising the event / text as a headline)\\n\"\n",
    "        \"- The start date of the event\\n\"\n",
    "        \"- The event type(s)\\n\"\n",
    "        \"- The province(s) in which the event occurred\\n\"\n",
    "        \"- The target group(s) of the event\\n\"\n",
    "        \"- The minimum number of people killed during the event\\n\"\n",
    "        \"- The minimum number of people captured during the event\\n\"\n",
    "        \"- Whether someone was killed or not during the event\\n\"\n",
    "        \"- Whether someone was captured or not during the event\\n\"\n",
    "        \"- Whether the event was a so-called 'kill-capture raid'\\n\"\n",
    "        \"- Whether an airstrike was used during the event\\n\"\n",
    "        \"- Whether no shots were fired during the event\\n\"\n",
    "        \"- The minimum number of leaders killed during the event\\n\"\n",
    "        \"- The minimum number of leaders captured during the event\\n\\n\"\n",
    "        \"## Annotation notes:\\n\"\n",
    "        \"- A 'faciliator' is not a leader.\\n\"\n",
    "        \"- If a press release states that 'insurgents' were detained without further \"\n",
    "        \"details, assign a minimum number of two detained. Interpret 'a couple' as \"\n",
    "        \"two. Interpret 'several' as at least three, even though it may sometimes \"\n",
    "        \"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a \"\n",
    "        \"small group', and 'multiple' as denoting at least three, even if they \"\n",
    "        \"sometimes refer to larger numbers. Choose the smaller number if no other \"\n",
    "        \"information is available in the press release to come up with a minimally \"\n",
    "        \"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, \"\n",
    "        \"and 'a large number' as at least five.\\n\\n\"\n",
    "        \"## Example:\\n\"\n",
    "        \"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \"\n",
    "        \"Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \"\n",
    "        \"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \"\n",
    "        \"their position talking on radios today. After gaining positive identification of the insurgent positions, the \"\n",
    "        \"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \"\n",
    "        \"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent \"\n",
    "        \"positions, resulting in several more insurgents being killed.'\\n\\n\"\n",
    "        'Output: `{\"name\":\"Several insurgents killed in '\n",
    "        'Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi'\n",
    "        'n_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"'\n",
    "        ':false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}`'\n",
    "    )\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url = 'http://localhost:11434/v1',\n",
    "        api_key='ollama', # required, but unused\n",
    "    )\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        async with session.post(\n",
    "            \"http://localhost:11434/v1/chat/completions\",\n",
    "            headers={\"Authorization\": f\"Bearer {client.api_key}\"},\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"response_format\": {\"type\": \"json_object\"},\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert at identifying events in a press release. You are precise \"\n",
    "                        \"and always make sure you are correct, drawing inference from the text of the \"\n",
    "                        \"press release.\\n\\n You always return a JSON string with the following schema: \"\n",
    "                        \"## JSON Schema details\\n\"\n",
    "                        \"Here is some of the schema for the JSON output string you \"\n",
    "                        \"should make use of: event_types = ['airstrike', 'detention', \"\n",
    "                        \"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], \"\n",
    "                        \"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', \"\n",
    "                        \"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', \"\n",
    "                        \"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', \"\n",
    "                        \"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', \"\n",
    "                        \"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', \"\n",
    "                        \"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', \"\n",
    "                        \"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": query},\n",
    "                ],\n",
    "                \"temperature\": 1,\n",
    "            },\n",
    "        ) as response:\n",
    "            result = await response.json()\n",
    "            if \"error\" in result:\n",
    "                error_message = result[\"error\"][\"message\"]\n",
    "                if \"Rate limit reached\" in error_message:\n",
    "                    # retry_delay_ms = float(\n",
    "                    #     error_message.split(\"Please try again in \")[1].split(\"ms\")[0]\n",
    "                    # )\n",
    "                    retry_delay_ms = 35000\n",
    "                    retry_delay_seconds = retry_delay_ms / 1000\n",
    "                    print(\n",
    "                        f\"Rate limit exceeded. Retrying in {retry_delay_seconds} seconds...\"\n",
    "                    )\n",
    "                    await asyncio.sleep(retry_delay_seconds)\n",
    "                    retries += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Error during prediction.\\nFull result object: {result}\")\n",
    "                    return \"\"\n",
    "            try:\n",
    "                return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            except KeyError:\n",
    "                print(f\"Error during prediction.\\nFull result object: {result}\")\n",
    "                return \"\"\n",
    "\n",
    "    print(f\"Max retries exceeded for event.\\nFull result object: {result}\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "async def get_gpt_predictions_async(\n",
    "    model: str,\n",
    "    events: List[IsafEvent],\n",
    "    logging_n: int = 100,\n",
    "    max_concurrent_requests: int = 5,\n",
    ") -> List[IsafEvent]:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "        tasks = []\n",
    "        for i, event in enumerate(events, start=1):\n",
    "            if i % logging_n == 0:\n",
    "                print(f\"Predicting event {i} of {len(events)} using {model}\")\n",
    "\n",
    "            async def make_request(session, event):\n",
    "                async with semaphore:\n",
    "                    return await async_query_openai(\n",
    "                        session, event.text, model, max_retries=5\n",
    "                    )\n",
    "\n",
    "            task = asyncio.ensure_future(make_request(session, event))\n",
    "            tasks.append(task)\n",
    "\n",
    "        predictions = await asyncio.gather(*tasks)\n",
    "        for event, prediction in zip(events, predictions):\n",
    "            event.predictions[model] = prediction\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "async def main():\n",
    "    events_4o = await get_gpt_predictions_async(\n",
    "        \"phi3\", events, max_concurrent_requests=10\n",
    "    )\n",
    "    events_4turbo = await get_gpt_predictions_async(\n",
    "        \"phi3-v2\", events_4o, max_concurrent_requests=10\n",
    "    )\n",
    "    full_events = await get_gpt_predictions_async(\n",
    "        \"gemma2\", events_4turbo, max_concurrent_requests=10\n",
    "    )\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def convert_to_dataset(data: List[IsafEvent]) -> Dataset:\n",
    "    names = []\n",
    "    texts = []\n",
    "    start_dates = []\n",
    "    provinces = []\n",
    "    target_groups = []\n",
    "    event_types = []\n",
    "    predictions = []\n",
    "    min_killeds = []\n",
    "    min_captureds = []\n",
    "    killqs = []\n",
    "    captureqs = []\n",
    "    killcaptureraids = []\n",
    "    airstrikes = []\n",
    "    noshotsfireds = []\n",
    "    min_leaders_killeds = []\n",
    "    min_leaders_captureds = []\n",
    "\n",
    "    for item in data:\n",
    "        names.append(item.name)\n",
    "        texts.append(item.text)\n",
    "        start_dates.append(item.start_date)\n",
    "        provinces.append(item.province)\n",
    "        target_groups.append(item.target_group)\n",
    "        event_types.append(item.event_type)\n",
    "        predictions.append(item.predictions)\n",
    "        min_killeds.append(item.min_killed)\n",
    "        min_captureds.append(item.min_captured)\n",
    "        killqs.append(item.killq)\n",
    "        captureqs.append(item.captureq)\n",
    "        killcaptureraids.append(item.killcaptureraid)\n",
    "        airstrikes.append(item.airstrike)\n",
    "        noshotsfireds.append(item.noshotsfired)\n",
    "        min_leaders_killeds.append(item.min_leaders_killed)\n",
    "        min_leaders_captureds.append(item.min_leaders_captured)\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"name\": names,\n",
    "        \"text\": texts,\n",
    "        \"predictions\": predictions,\n",
    "        \"start_date\": start_dates,\n",
    "        \"province\": provinces,\n",
    "        \"target_group\": target_groups,\n",
    "        \"event_type\": event_types,\n",
    "        \"min_killed\": min_killeds,\n",
    "        \"min_captured\": min_captureds,\n",
    "        \"killq\": killqs,\n",
    "        \"captureq\": captureqs,\n",
    "        \"killcaptureraid\": killcaptureraids,\n",
    "        \"airstrike\": airstrikes,\n",
    "        \"noshotsfired\": noshotsfireds,\n",
    "        \"min_leaders_killed\": min_leaders_killeds,\n",
    "        \"min_leaders_captured\": min_leaders_captureds,\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_and_push_dataset(\n",
    "    events: List[IsafEvent], name: str, split_name: str = \"train\"\n",
    "):\n",
    "    \"\"\"Convert a list of Pydantic objects to a HF Dataset object, then push to\n",
    "    the hub.\"\"\"\n",
    "    hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "    dataset = convert_to_dataset(events)\n",
    "    dataset.push_to_hub(\n",
    "        f\"strickvl/{name}\",\n",
    "        token=hf_token,\n",
    "        private=True,\n",
    "        create_pr=True,\n",
    "        split=split_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(data: List[BaseModel]) -> Dataset:\n",
    "    dataset_dict = {}\n",
    "\n",
    "    for field_name, field_value in data[0].__fields__.items():\n",
    "        field_type = field_value.outer_type_\n",
    "        if field_type in [str, int, float, bool, date]:\n",
    "            dataset_dict[field_name] = [getattr(item, field_name) for item in data]\n",
    "        elif field_type == set:\n",
    "            dataset_dict[field_name] = [list(getattr(item, field_name)) for item in data]\n",
    "        elif issubclass(field_type, BaseModel):\n",
    "            dataset_dict[field_name] = [getattr(item, field_name).dict() for item in data]\n",
    "        else:\n",
    "            dataset_dict[field_name] = [getattr(item, field_name) for item in data]\n",
    "\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_and_push_dataset(events, \"isafpressreleases_with_preds\", split_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "preds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds\")[\n",
    "    \"test\"\n",
    "].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def prompt(press_release: str) -> str:\n",
    "    return f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "PRESS RELEASE TEXT: \"{press_release}\"\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prompt_tok(\n",
    "    model: AutoPeftModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    press_release: str,\n",
    "    return_ids: bool = False,\n",
    ") -> Union[str, torch.Tensor]:\n",
    "    _p = prompt(press_release)\n",
    "    input_ids = tokenizer(_p, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    out_ids = model.generate(input_ids=input_ids, max_new_tokens=5000, do_sample=False)\n",
    "    ids = out_ids.detach().cpu().numpy()\n",
    "    if return_ids:\n",
    "        return out_ids\n",
    "    return tokenizer.batch_decode(ids, skip_special_tokens=True)[0][len(_p) :]\n",
    "\n",
    "\n",
    "tinyllama_sharegpt_model_id = \"strickvl/isafpr-tiny-llama-lora-templatefree\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(tinyllama_sharegpt_model_id).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(tinyllama_sharegpt_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "for row in preds_test_data:\n",
    "    out = prompt_tok(model, tokenizer, row[\"text\"])\n",
    "    row[\"predictions\"][\"tinyllama-templatefree\"] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "print(preds_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuned Llama3 predictions (via OpenPipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "preds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds_2\")[\n",
    "    \"train\"\n",
    "].to_list()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "for row in preds_test_data:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ft:gpt-3.5-turbo-1106:SOME_MODEL_ID_GOES_HERE\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": row[\"text\"]},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    row[\"predictions\"][\"finetuned-openai-gpt-3.5-turbo-1106\"] = response.choices[\n",
    "        0\n",
    "    ].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "preds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds_3\")[\n",
    "    \"train\"\n",
    "].to_list()\n",
    "\n",
    "client = OpenAI(openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")})\n",
    "\n",
    "for row in preds_test_data:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"openpipe:fine-steaks-taste\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": row[\"text\"]},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        openpipe={\"tags\": {\"prompt_id\": \"counting\", \"any_key\": \"any_value\"}},\n",
    "    )\n",
    "\n",
    "    row[\"predictions\"][\"finetuned-llama3-7b-32k-openpipe\"] = completion.choices[\n",
    "        0\n",
    "    ].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "preds_test_data = load_dataset(\"strickvl/isafpressreleases_test_predictions_old\")[\n",
    "    \"train\"\n",
    "].to_list()\n",
    "\n",
    "client = OpenAI(openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")})\n",
    "\n",
    "for i, row in enumerate(preds_test_data, 1):\n",
    "    completion_7b = client.chat.completions.create(\n",
    "        model=\"openpipe:twelve-pumas-invent\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": row[\"text\"]},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        openpipe={\"tags\": {\"prompt_id\": \"counting\", \"any_key\": \"any_value\"}},\n",
    "    )\n",
    "\n",
    "    row[\"predictions\"][\"finetuned-mistral-7b-optimised-openpipe\"] = completion_7b.choices[0].message.content\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/724 rows complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_with_preds = load_dataset(\"strickvl/isafpressreleases_test_predictions\")[\n",
    "    \"train\"\n",
    "].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "json_aggregate_scores = {\n",
    "    \"gpt-4o\": 0,\n",
    "    \"gpt-4-turbo\": 0,\n",
    "    \"gpt-3.5-turbo\": 0,\n",
    "    \"tinyllama-templatefree\": 0,\n",
    "    \"tinyllama-sharegpt\": 0,\n",
    "    \"finetuned-openai-gpt-3.5-turbo-1106\": 0,\n",
    "    \"finetuned-llama3-7b-32k-openpipe\": 0,\n",
    "    \"mistral-lora-templatefree\": 0,\n",
    "    \"finetuned-mistral-7b-optimised-openpipe\": 0,\n",
    "    \"ft-solar-1-mini-chat-240612-predibase\": 0,\n",
    "}\n",
    "\n",
    "for row in dataset_with_preds:\n",
    "    for model in row[\"predictions\"]:\n",
    "        try:\n",
    "            json.loads(row[\"predictions\"][model])\n",
    "            json_aggregate_scores[model] += 1\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            pass\n",
    "\n",
    "# print(json_aggregate_scores)\n",
    "\n",
    "# Separate GPT models and finetuned models\n",
    "gpt_models = [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]\n",
    "finetuned_models = [\n",
    "    model for model in json_aggregate_scores.keys() if model not in gpt_models\n",
    "]\n",
    "\n",
    "# Create lists for plotting\n",
    "models = list(json_aggregate_scores.keys())\n",
    "scores = list(json_aggregate_scores.values())\n",
    "colors = [\"#1f77b4\" if model in gpt_models else \"#ff7f0e\" for model in models]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot horizontal bars\n",
    "bars = ax.barh(models, scores, color=colors)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Number of Valid JSON Outputs\")\n",
    "ax.set_title(\"Valid JSON Outputs by Model\")\n",
    "ax.set_xlim(0, 750)  # Set x-axis limit to slightly above max score\n",
    "\n",
    "# Reduce font size for y-axis labels (model names)\n",
    "ax.tick_params(axis=\"y\", labelsize=8)\n",
    "\n",
    "# Add value labels at the end of each bar\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width, bar.get_y() + bar.get_height() / 2, f\"{width}\", ha=\"left\", va=\"center\"\n",
    "    )\n",
    "\n",
    "# Create custom legend handles\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#ff7f0e\", label=\"Finetuned Models\"),\n",
    "    Patch(facecolor=\"#1f77b4\", label=\"GPT Models\"),\n",
    "]\n",
    "\n",
    "# Add a legend outside the plot\n",
    "ax.legend(handles=legend_elements, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust layout to prevent clipping and make room for the legend\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
